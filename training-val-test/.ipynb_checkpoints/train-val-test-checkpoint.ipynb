{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3d9f2e-3887-4434-a30a-a56d614b2783",
   "metadata": {},
   "source": [
    "# InSAR Denoiser training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13c64b7-0356-4882-8a0a-ff3bb7ca899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c08-6943-429d-a3d8-484663017e37",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "intended file structure to ingest includes four dirs: train_image, train_label, test_image. Folders contain only images. \n",
    "- Filenames in image folders: sub1_S1AA_20190505T135154_20190622T135157_VVP048_INT80_G_weF_011A_los_disp.tif\n",
    "- Filenames in label folders: sub1_S1AA_20190505T135154_20190622T135157_VVP048_INT80_G_weF_011A_los_disp.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dc6ed2-c3e1-4a93-ac8b-363633956f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = os.listdir('/Users/qbren/Desktop/uw_courses/2022_spring/inferring-structure/project/data_processing/data_crop')\n",
    "test_fns = os.listdir('/Users/qbren/Desktop/uw_courses/2022_spring/inferring-structure/project/data_processing/data_crop') #change paths before use\n",
    "\n",
    "def keep_tifs(my_fns):\n",
    "    my_list = []\n",
    "    for i in my_fns:\n",
    "        if i[-4:] == '.tif':\n",
    "            my_list.append(i)\n",
    "    return my_list\n",
    "        \n",
    "    \n",
    "train_list = keep_tifs(train_fns)\n",
    "test_list = keep_tifs(test_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dcde0f9-6fc5-4bed-9a1a-2871cd07750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, val_list = train_test_split(train_list, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc438ce1-31a0-482d-a684-e1bc74217332",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor() #because label is also an image that needs to match, can't do any flipping\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40cf13d3-55c8-44da-b7d6-665a717f74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization between -1 and 1 as in Zhao et al. https://doi.org/10.1016/j.isprsjprs.2021.08.009\n",
    "\n",
    "train_img_dir = '/Users/qbren/Desktop/uw_courses/2022_spring/inferring-structure/project/data_processing/data_crop/'\n",
    "train_label_dir = ''\n",
    "test_img_dir = ''\n",
    "test_label_dir = ''\n",
    "\n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,file_list, img_dir, label_dir, transform=None, norm=True):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.norm = norm\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        img_path = self.img_dir+self.file_list[idx]\n",
    "        label_path = self.label_dir+self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        label = Image.open(label_path)\n",
    "        img_transformed = self.transform(img)\n",
    "        label_transformed = self.transform(label)\n",
    "        \n",
    "        # Perform normalization\n",
    "        if self.norm == True:\n",
    "            img_transformed = 2*(((img_transformed-img_transformed.min())/(img_transformed.max()-img_transformed.min())))-1\n",
    "            label_transformed = 2*(((label_transformed-label_transformed.min())/(label_transformed.max()-label_transformed.min())))-1 \n",
    "        \n",
    "        return img_transformed, label_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4556228-d7e7-4316-8040-3d32c8864bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data = dataset(train_list, train_img_dir, train_label_dir, transform=my_transforms)\n",
    "val_data = dataset(val_list, train_img_dir, train_label_dir, transform=my_transforms)\n",
    "test_data = dataset(test_list, test_img_dir, test_label_dir, transform=my_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=64, shuffle=True )\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915daafd-ecb3-4561-8d38-b3f452cef706",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1acb7d19-044f-4aaa-a311-0ce16f8298af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from Zhang et al. https://doi.org/10.1109/TIP.2017.2662206, as implemented here: https://github.com/SaoYan/DnCNN-PyTorch \n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, channels, num_of_layers=17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        features = 64\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(num_of_layers-2):\n",
    "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.dncnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413fa05-2eef-409d-8421-115ef2713b8d",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec010c-58d6-4c87-94af-db69780037ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Define optimizer\n",
    "model = DnCNN(channels=1)\n",
    "model.to('cuda') # run on gpu\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #original implementation reduces learning rate at milestone 30 epochs\n",
    "loss_fn   = nn.MSELoss() \n",
    "epochs = 30\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nstarting epoch {epoch}')\n",
    "    epoch_loss=[]\n",
    "    val_temp_loss = []\n",
    "    \n",
    "    #loop through training data \n",
    "    for (sample, target) in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = torch.clamp(sample.to('cuda') - model(sample.to('cuda')), -1, 1) #Generate noise predictions using the model, subtract from interferogram\n",
    "        loss = loss_fn(out.to('cuda'), target.to('cuda')) #Loss/error\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward() #Propagate the gradients in backward pass\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    print(f'training loss: {np.mean(epoch_loss)}')\n",
    "    \n",
    "    # run model on validation data \n",
    "    for (sample, target) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = torch.clamp(sample.to('cuda') - model(sample.to('cuda')), -1, 1) #Generate predictions using the model\n",
    "            loss = loss_fn(out.to('cuda'), target.to('cuda')) #Loss/error\n",
    "            val_temp_loss.append(loss.item())\n",
    "    \n",
    "    valid_loss.append(np.mean(val_temp_loss))\n",
    "    print(f'validation loss: {np.mean(val_temp_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084aa62b-a5c4-4ad0-ba63-02997201beaa",
   "metadata": {},
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd788d8-aa18-438b-860f-103f48284a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(v1_train_loss, label='training')\n",
    "ax.plot(v1_valid_loss, label='validaton')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('MSE loss')\n",
    "ax.set_title('Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3709e4d-00a1-49be-b1ea-0e5f365611ff",
   "metadata": {},
   "source": [
    "### Visualize outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1d166-1f04-481a-a192-ee8250bf3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=1, shuffle=True) #change batch size\n",
    "\n",
    "for i, (sample, target) in enumerate(val_loader):\n",
    "    if i < num_images:\n",
    "        with torch.no_grad():\n",
    "            noise = model(sample.to('cuda')) #Generate predictions using the model\n",
    "            nn_corrected = torch.clamp(sample.to('cuda') - noise, -1, 1)\n",
    "            \n",
    "            f, ax = plt.subplots(columns=4, figsize=(5,15))\n",
    "            ax[0].imshow(sample.permute(1, 2, 0)) #will probably need to fix this\n",
    "            ax[0].set_title('original interferogram')\n",
    "            ax[1].imshow(noise.permute(1, 2, 0))\n",
    "            ax[1].set_title('predicted noise')\n",
    "            ax[2].imshow(nn_corrected.permute(1, 2, 0))\n",
    "            ax[2].set_title('NN corrected interferogram')\n",
    "            ax[3].imshow(target.permute(1, 2, 0))\n",
    "            ax[3].set_title('ERA5 corrected interferogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175e2cd-281b-4571-88c2-570cb6b9604d",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca1e42-7a55-4683-a840-161ef696ef4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
